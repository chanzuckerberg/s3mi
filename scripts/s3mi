#!/usr/bin/env python
#
# Tested under python 2.7, 3.5, 3.6.
#
# Copyright (c) 2017 Chan Zuckerberg Initiative, see LICENSE.

import threading
import multiprocessing
import subprocess
import mmap
import json

try:
    from Queue import Queue
except:
    from queue import Queue

import os
import sys
import time
import traceback

try:
    from subprocess import DEVNULL
except:
    DEVNULL = open(os.devnull, "r+b")


help_text = """
S3MI:  Download huge files from S3 to EC2, fast.

Usage:
    https://github.com/chanzuckerberg/s3mi/blob/master/README.md

License:
    https://github.com/chanzuckerberg/s3mi/blob/master/LICENSE
"""

# The goal is to stream from S3 at 2GB/sec.
# As each request is limited to 1 Gbit/sec, we need 16+ concurrent requests.
# Too many and we risk running out of ports or other system resources.
MAX_CONCURRENT_REQUESTS = 24

# Max RAM required = SEGMENT_SIZE * MAX_SEGMENTS_IN_RAM
SEGMENT_SIZE = 512*1024*1024
MAX_SEGMENTS_IN_RAM = 64


MAX_CONCURRENT_REQUESTS = 2
SEGMENT_SIZE = 256*1024
MAX_SEGMENTS_IN_RAM = 5



# Max time in seconds without a single chunk completing its fetch.
TIMEOUT = 120


def tsprint(msg):
    sys.stderr.write(msg)
    sys.stderr.write("\n")


def num_segments(file_size):
    return (file_size + SEGMENT_SIZE - 1) // SEGMENT_SIZE


def segment_start(n, N, file_size):
    return min(SEGMENT_SIZE * n, file_size)


def part_filename(destination, n, N):
    return "part.{N}.{n:06d}.{destination}".format(destination=destination, N=N, n=n)


def safe_remove(f):
    try:
        if os.path.exists(f):
            os.remove(f)
    except:
        pass


def get_file_size(s3_uri):
    command_str = "aws s3 ls {s3_uri}".format(s3_uri=s3_uri)
    tsprint(command_str)
    result = subprocess.check_output(command_str.split())
    return int(result.split()[2])


def s3_bucket_and_key(s3_uri):
    prefix = "s3://"
    assert s3_uri.startswith(prefix)
    return s3_uri[len(prefix):].split("/", 1)


def note_error(errors):
    with errors[0]:
        first_error = (errors[1] == 0)
        errors[1] += 1
    if first_error:
        traceback.print_exc()


mlock = multiprocessing.Lock()

def mmap_write(mm, first, part):
    with open(part, 'rb') as pf:
        segment_bytes = pf.read()
    safe_remove(part)
    # mm[first : first + len(segment_bytes)] = segment_bytes
    with mlock:
        mm.seek(first)
        mm.write(segment_bytes)


def fetch(mm, s3_bucket, s3_key, part, n, N, file_size, request_tokens, buffer_tokens, errors):
    try:
        try:
            first = segment_start(n, N, file_size)
            last = segment_start(n + 1, N, file_size) - 1  # aws api wants inclusive bounds
            command_str = "aws s3api get-object --range bytes={rfrom}-{rto} --bucket {bucket} --key {key} {part}".format(
                rfrom=first, rto=last, bucket=s3_bucket, key=s3_key, part=part)
            safe_remove(part)
            os.mkfifo(part)
            try:
                watchdog = None
                fetcher_subproc = subprocess.Popen(command_str.split(), stdout=DEVNULL)
                watchdog = threading.Timer(TIMEOUT, fetcher_subproc.terminate)
                watchdog.start()
                writer = multiprocessing.Process(target=mmap_write, args=[mm, first, part])
                writer.start()
                fetcher_subproc.wait()
            except:
                note_error(errors)
            finally:
                if watchdog:
                    watchdog.cancel()
                returncode = fetcher_subproc.poll()
                if returncode != 0:
                    if returncode == None:
                        fetcher_subproc.terminate()
                    raise Exception("Error {} in s3api get-object for part {}".format(returncode or 'unknown', n))
        finally:
            request_tokens.release()
        writer.join(TIMEOUT)
        if writer.exitcode != 0:
            raise Exception("Error {} in mmap write for part {}".format(writer.exitcode or 'timeout', n))
    except:
        note_error(errors)
    finally:
        buffer_tokens.release()


def main_cp(s3_uri, destination):
    errors = [threading.RLock(), 0]
    try:
        pid = os.getpid()
        download = destination + ".download"
        safe_remove(download)
        with open(download, "wb") as dest:
            dest.write("hello")
        file_size = get_file_size(s3_uri)
        tsprint("File size is {:3.1f} GB ({} bytes).".format(file_size/(2**30), file_size))
        s3_bucket, s3_key = s3_bucket_and_key(s3_uri)
        N = num_segments(file_size)
        tsprint("Fetching {} segments.".format(N))
        with open(download, "r+b") as dest:
            # os.ftrunctate instead of mmap.resize because mmap.resize is not implemented everywhere
            os.ftruncate(dest.fileno(), file_size)
            mm = mmap.mmap(dest.fileno(), 0)
            try:
                threads = []
                request_tokens = threading.Semaphore(MAX_CONCURRENT_REQUESTS)
                buffer_tokens =  threading.Semaphore(MAX_SEGMENTS_IN_RAM)
                for n in range(N):
                    request_tokens.acquire()
                    buffer_tokens.acquire()
                    part = part_filename("download-{}".format(pid), n, N)
                    t = threading.Thread(
                        target=fetch,
                        args=[mm, s3_bucket, s3_key, part, n, N, file_size, request_tokens, buffer_tokens, errors]
                    )
                    t.start()
                    threads.append(t)
            except:
                note_error(errors)
            finally:
                for t in threads:
                    t.join()
                mm.close()
    except:
        note_error(errors)
    finally:
        with errors[0]:
            has_error = errors[1] > 0
        if has_error:
            for n in range(N):
                safe_remove(part_filename("download-{}".format(pid), n, N))
            safe_remove(download)
            return 1
    os.rename(download, destination)
    return 0


def main_raid(volume_name, *optional_args):
    # fixme: Deal with rate-limited API
    # fixme: Any sort of error recovery
    if len(optional_args) >= 1:
        N = int(optional_args[0])
    else:
        # good for 1750 MB/sec EBS-optimized instance
        N = 11
    if len(optional_args) >= 2:
        slice_size = int(optional_args[1])
    else:
        # for gp2 volume to be able to maintain 160 MB/sec
        slice_size = 214
    mountpoint = "/mnt/{}".format(volume_name)
    try:
        if os.path.exists(mountpoint):
            subprocess.call("sudo rmdir {}".format(mountpoint).split())
    except:
        traceback.print_exc()
        tsprint("Mountpoint directory {} exists and not empty.".format(mountpoint))
        return 1
    subprocess.check_output("sudo mkdir {}".format(mountpoint).split())
    volume_ids = []
    availability_zone = instance_availability_zone()
    for n in range(N):
        slice_name = "{vn}_{N}_{n}".format(vn=volume_name, N=N, n=n)
        tsprint("Creating slice {vn} size {sz} in availability zone {az}"
                .format(vn=slice_name, sz=slice_size, az=availability_zone))
        vid = create_volume(slice_name, slice_size, availability_zone)
        volume_ids.append(vid)
    tsprint("Waiting for all {N} slices to become available".format(N=N))
    def available(v):
        return v["State"] == "available"
    if not wait_until_state(volume_ids, available):
        tsprint("Timeout")
        return 1
    iid = instance_id()
    tsprint("Attaching slices to instance {}".format(iid))
    theoretical_devices = set("xvd" + chr(i) for i in range(ord('a'), ord('z')+1))
    occupied_devices = set(os.listdir("/dev")) & theoretical_devices
    available_devices = theoretical_devices - occupied_devices
    suggested_devices = sorted(available_devices)[:N]
    for vid, devnode in zip(volume_ids, suggested_devices):
        command = "aws ec2 attach-volume --instance-id {iid} --volume-id {vid} --device {devnode}"
        command = command.format(iid=iid, vid=vid, devnode=devnode)
        subprocess.check_output(command.split())
    def attached_to_instance(v):
        return v["State"] == "in-use" and v["Attachments"] and v["Attachments"][0]["InstanceId"] == iid
    if not wait_until_state(volume_ids, attached_to_instance):
        tsprint("Timeout")
        return 1
    occupied_devices_2 = set(os.listdir("/dev")) & theoretical_devices
    new_devices = sorted(occupied_devices_2 - occupied_devices)
    if len(new_devices) != N:
        tsprint("Some other process is attaching/detaching volumes to instance {}".format(iid))
        return 1
    tsprint("Initializing software RAID-0")
    md = sorted(set("md{}".format(i) for i in range(10)) - set(os.listdir("/dev")))[0]
    # 256KB chunk is great for EBS
    command = "sudo mdadm --create --verbose /dev/{md} --level=0 --chunk 256 --name={vn} --raid-devices={N} "
    command += " ".join("/dev/{}".format(devnode) for devnode in new_devices)
    command = command.format(md=md, vn=volume_name, N=N)
    subprocess.check_output(command.split())
    # This is recommended by Amazon documentation.
    subprocess.check_output("sudo sysctl dev.raid.speed_limit_min=30720".split())
    # without -E nodiscard it will take forever to initialize the entire space
    subprocess.check_output("sudo mkfs.ext4 -E nodiscard /dev/{md}".format(md=md).split())
    subprocess.check_output("sudo mount /dev/{} {}".format(md, mountpoint).split())
    user = subprocess.check_output(["whoami"]).strip()
    subprocess.check_output("sudo chown -R {} {}".format(user, mountpoint).split())
    tsprint("Success")
    return 0


def initiate_fetch(s3_bucket, s3_key, part, n, N, file_size, request_tokens, errors):
    fetcher_subproc = None
    watchdog = None
    def kill():
        if fetcher_subproc:
            fetcher_subproc.terminate()
    def note_error():
        with errors[0]:
            errors[1] += 1
        tsprint("Fetching part '{}' failed.".format(part))
    def wait_and_release():
        try:
            if fetcher_subproc and fetcher_subproc.wait() != 0:
                note_error()
        except:
            note_error()
            raise
        finally:
            request_tokens.release()
            if watchdog:
                watchdog.cancel()
    try:
        first = segment_start(n, N, file_size)
        last = segment_start(n + 1, N, file_size) - 1  # aws api wants inclusive bounds
        command_str = "aws s3api get-object --range bytes={rfrom}-{rto} --bucket {bucket} --key {key} {part}".format(
            rfrom=first, rto=last, bucket=s3_bucket, key=s3_key, part=part)
        safe_remove(part)
        os.mkfifo(part)
        fetcher_subproc = subprocess.Popen(command_str.split(), stdout=DEVNULL)
    except:
        with errors[0]:
            errors[1] += 1
    finally:
        if fetcher_subproc:
            watchdog = threading.Timer(TIMEOUT, kill)
            watchdog.start()
        threading.Thread(target=wait_and_release).start()


def append(part, baton):
    with open(part, 'rb') as pf:
        segment_bytes = pf.read()
    baton.acquire()
    sys.stdout.write(segment_bytes)


def main_cat(s3_uri, do_not_reopen=False):
    file_size = get_file_size(s3_uri)
    tsprint("File size is {:3.1f} GB ({} bytes).".format(file_size/(2**30), file_size))
    s3_bucket, s3_key = s3_bucket_and_key(s3_uri)
    N = num_segments(file_size)
    tsprint("Fetching {} segments.".format(N))
    active_appenders = Queue(MAX_SEGMENTS_IN_RAM)
    request_tokens = threading.Semaphore(MAX_CONCURRENT_REQUESTS)
    errors = [threading.RLock(), 0]
    if not do_not_reopen:
        sys.stdout = os.fdopen(sys.stdout.fileno(), 'ab')
    def error_state():
        with errors[0]:
            return errors[1]
    def baton_passer_loop():
        while True:
            part, appender, baton = active_appenders.get()
            if appender == None:
                break
            try:
                baton.release()
                t0 = time.time()
                while appender.is_alive() and not error_state() and time.time() - t0 < TIMEOUT:
                    appender.join(5.0)
                if error_state() and appender.is_alive():
                    appender.terminate()
                assert appender.exitcode == 0
            except:
                with errors[0]:
                    errors[1] += 1
                tsprint("Error appending part '{}'.".format(part))
            finally:
                safe_remove(part)
    baton_passer = threading.Thread(target=baton_passer_loop)
    baton_passer.start()
    pid = os.getpid()
    try:
        for n in range(N):
            request_tokens.acquire()
            if error_state():
                break
            part = part_filename("download-{}".format(pid), n, N)
            baton = multiprocessing.Semaphore(1)
            baton.acquire()
            initiate_fetch(s3_bucket, s3_key, part, n, N, file_size, request_tokens, errors)
            appender = multiprocessing.Process(target=append, args=[part, baton])
            appender.start()
            active_appenders.put((part, appender, baton), block=True, timeout=TIMEOUT)
    finally:
        active_appenders.put((None, None, None))
        baton_passer.join()
        for n in range(N):
            safe_remove(part_filename("download-{}".format(pid), n, N))
    if error_state():
        return 1
    return 0


def instance_availability_zone():
    "Return availability zone of current instance."
    return subprocess.check_output("curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone".split())


def instance_id():
    "Return current instance id."
    return subprocess.check_output("curl -s http://169.254.169.254/latest/meta-data/instance-id".split())


def create_volume(volume_name, volume_size, availability_zone, volume_type="gp2"):
    "Return volume id of newly created volume.  The volume may not be available yet."
    command = ("aws ec2 create-volume --volume-type {vt} --size {size} " +
               "--tag-specifications ResourceType=volume,Tags=[{{Key=Name,Value={vn}}}] --availability-zone {az}")
    command = command.format(vn=volume_name, az=availability_zone, size=volume_size, vt=volume_type)
    props = json.loads(subprocess.check_output(command.split()))
    return props["VolumeId"]


def wait_until_state(volume_ids, predicate, timeout=300):
    "Wait up to timeout for all volume_ids to become available.  If the timeout expires, return False."
    command = "aws ec2 describe-volumes --volume-ids " + " ".join(volume_ids)
    t0 = time.time()
    sleep_quantum = 15.0
    while True:
        time.sleep(sleep_quantum)
        props = json.loads(subprocess.check_output(command.split()))
        vstate = {}
        for v in props["Volumes"]:
            vstate[v["VolumeId"]] = predicate(v)
        if all((vid in vstate and vstate[vid]) for vid in volume_ids):
            # Success
            return True
        remaining_time = timeout - (time.time() - t0)
        if remaining_time < sleep_quantum:
            # Timeout
            return False


def main(argv):
    assert len(argv) >= 2
    s3mi, command, args = argv[0], argv[1], argv[2:]
    assert s3mi.endswith("s3mi")
    if command == "cp":
        result = main_cp(*args)
    elif command == "cat":
        result = main_cat(*args)
    elif command == "raid":
        result = main_raid(*args)
    else:
        raise Exception("Unsupported command '{}', see usage.".format(command))
    return result


if __name__ == "__main__":
    try:
        exitcode = main(sys.argv)
        if exitcode != 0:
            sys.exit(exitcode)
    except:
        traceback.print_exc()
        tsprint(help_text)
        sys.exit(1)
