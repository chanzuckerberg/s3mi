#!/usr/bin/env python
#
# Tested under python 2.7, 3.5, 3.6.
#
# Copyright (c) 2017 Chan Zuckerberg Initiative, see LICENSE.

import threading
import multiprocessing
import json

try:
    from Queue import Queue
except:
    from queue import Queue

import os
import sys
import time
import traceback

import subprocess
try:
    from subprocess import DEVNULL
except:
    DEVNULL = open(os.devnull, "r+b")


help_text = """
S3MI:  Download huge files from S3 to EC2, fast.

Usage:
    https://github.com/chanzuckerberg/s3mi/blob/master/README.md

License:
    https://github.com/chanzuckerberg/s3mi/blob/master/LICENSE
"""


# The goal is to stream from S3 at 2GB/sec.
# As each request is limited to 60 MB/sec, we need 34+ concurrent requests.
# Too many and we risk running out of ports or other system resources.
MAX_CONCURRENT_REQUESTS = 36


# Max RAM required = SEGMENT_SIZE * MAX_SEGMENTS_IN_RAM
# These defaults may be overridden based on available_gigs_of_RAM, see below.
SEGMENT_SIZE = 384*1024*1024
MAX_SEGMENTS_IN_RAM = 72


# Max time in seconds without a single chunk completing its fetch.
TIMEOUT = 120


def tsprint(msg):
    sys.stderr.write(msg)
    sys.stderr.write("\n")


def num_segments(file_size):
    return (file_size + SEGMENT_SIZE - 1) // SEGMENT_SIZE


def segment_start(n, file_size):
    return min(SEGMENT_SIZE * n, file_size)


def part_filename(destination, n, N):
    return "part.{N}.{n:06d}.{destination}".format(destination=destination, N=N, n=n)


def safe_remove(f):
    try:
        if os.path.exists(f):
            os.remove(f)
    except:
        pass


def check_output(command):
    # Compatibility with both 2.7 and 3.7 requires this bizarre thing.
    return subprocess.check_output(command).decode()


def get_file_size(s3_uri):
    command_str = "aws s3 ls {s3_uri}".format(s3_uri=s3_uri)
    tsprint(command_str)
    result = check_output(command_str.split())
    return int(result.split()[2])


def s3_bucket_and_key(s3_uri):
    prefix = "s3://"
    assert s3_uri.startswith(prefix)
    return s3_uri[len(prefix):].split("/", 1)


def main_cp(s3_uri, destination):
    tsprint("Note:  This version of 's3mi cp' uses 's3 cat'.  TODO: Use mmap.")
    if os.path.exists(destination) and os.path.isdir(destination):
        filename = s3_uri.rsplit("/", 1)[-1]
        destination = destination + "/" + filename
    download = destination + ".download"
    safe_remove(download)
    try:
        with open(download, "ab") as dest:
            sys.stdout = dest
            exitcode = main_cat(s3_uri)
        if exitcode == 0:
            os.rename(download, destination)
        return exitcode
    finally:
        safe_remove(download)


def make_mountpoint(volume_name):
    mountpoint = "/mnt/{}".format(volume_name)
    try:
        if os.path.exists(mountpoint):
            subprocess.call("sudo rmdir {}".format(mountpoint).split())
    except:
        traceback.print_exc()
        tsprint("Mountpoint directory {} exists and not empty.".format(mountpoint))
        return 1
    check_output("sudo mkdir {}".format(mountpoint).split())
    return mountpoint


def first_available_md_device_node():
    return sorted(set("md{}".format(i) for i in range(10)) - set(os.listdir("/dev")))[0]


def main_raid_ebs(volume_name, *optional_args):
    # fixme: Deal with rate-limited API
    # fixme: Any sort of error recovery
    if len(optional_args) >= 1:
        N = int(optional_args[0])
    else:
        # good for 1750 MB/sec EBS-optimized instance
        N = 11
    if len(optional_args) >= 2:
        slice_size = int(optional_args[1])
    else:
        # for gp2 volume to be able to maintain 160 MB/sec
        slice_size = 334
    mountpoint = make_mountpoint(volume_name)
    volume_ids = []
    availability_zone = instance_availability_zone()
    for n in range(N):
        slice_name = "{vn}_{N}_{n}".format(vn=volume_name, N=N, n=n)
        tsprint("Creating slice {vn} size {sz} in availability zone {az}"
                .format(vn=slice_name, sz=slice_size, az=availability_zone))
        vid = create_volume(slice_name, slice_size, availability_zone)
        volume_ids.append(vid)
    tsprint("Waiting for all {N} slices to become available".format(N=N))
    def available(v):
        return v["State"] == "available"
    if not wait_until_state(volume_ids, available):
        tsprint("Timeout")
        return 1
    iid = instance_id()
    tsprint("Attaching slices to instance {}".format(iid))
    theoretical_devices = set("xvd" + chr(i) for i in range(ord('a'), ord('z')+1))
    occupied_devices = set(os.listdir("/dev")) & theoretical_devices
    available_devices = theoretical_devices - occupied_devices
    suggested_devices = sorted(available_devices)[:N]
    for vid, devnode in zip(volume_ids, suggested_devices):
        command = "aws ec2 attach-volume --instance-id {iid} --volume-id {vid} --device {devnode}"
        command = command.format(iid=iid, vid=vid, devnode=devnode)
        check_output(command.split())
    def attached_to_instance(v):
        return v["State"] == "in-use" and v["Attachments"] and v["Attachments"][0]["InstanceId"] == iid
    if not wait_until_state(volume_ids, attached_to_instance):
        tsprint("Timeout")
        return 1
    occupied_devices_2 = set(os.listdir("/dev")) & theoretical_devices
    raidable_devices = sorted(occupied_devices_2 - occupied_devices)
    if len(raidable_devices) != N:
        # Nitro instances are slightly weird.  To attach the devices you have to call them xvda, xvdb, etc.
        # But to raid them you must call them nvme1n1, nvme2n1, etc.  Also - they do not show up in ls /dev.
        assert occupied_devices_2 == occupied_devices, "sorry, something weird going on -- read the code."
        raidable_devices = ["nvme{}n1".format(i) for i in range(1, N + 1)]
    tsprint("Initializing software RAID-0")
    md = first_available_md_device_node()
    # 256KB chunk is great for EBS
    command = "sudo mdadm --create --run --verbose /dev/{md} --level=0 --chunk 256 --name={vn} --raid-devices={N} "
    command += " ".join("/dev/{}".format(devnode) for devnode in raidable_devices)
    command = command.format(md=md, vn=volume_name, N=N)
    check_output(command.split())
    # This is recommended by Amazon documentation.
    check_output("sudo sysctl dev.raid.speed_limit_min=30720".split())
    # without -K it will take forever to initialize the entire space
    # sunit = RAID chunk size in units of 512 bytes
    # swidth = sunit * number of slices
    check_output("sudo mkfs.xfs -K  -d sunit=512,swidth={sw} /dev/{md}".format(md=md, sw=512*N).split())
    check_output("sudo mount /dev/{} {}".format(md, mountpoint).split())
    user = os.getenv("SUDO_USER", check_output(["whoami"]).strip())
    check_output("sudo chown -R {} {}".format(user, mountpoint).split())
    tsprint("Success")
    return 0


def main_raid_nvme(volume_name, *optional_args):
    devices = []
    for devnode in optional_args:
        if not devnode.startswith("/dev/") or not os.path.exists(devnode):
            tsprint("Device {dn} does not exist.".format(dn=devnode))
            return 1
        devices.append(devnode)
    N = len(devices)
    if N < 2:
        tsprint("Fewer than 2 devices specified.")
        return 2
    mountpoint = make_mountpoint(volume_name)
    md = first_available_md_device_node()
    command = "sudo mdadm --create --run --verbose /dev/{md} --level=0 --chunk 128 --name={vn} --raid-devices={N} "
    command += " ".join(devices)
    command = command.format(md=md, vn=volume_name, N=N)
    check_output(command.split())
    check_output("sudo sysctl dev.raid.speed_limit_min=30720".split())
    # without -K it will take forever to initialize the entire space
    # sunit = RAID chunk size in units of 512 bytes
    # swidth = sunit * number of slices
    check_output("sudo mkfs.xfs -f -K -d sunit=256,swidth={sw} /dev/{md}".format(md=md, sw=256*N).split())
    check_output("sudo mount /dev/{} {}".format(md, mountpoint).split())
    user = os.getenv("SUDO_USER", check_output(["whoami"]).strip())
    check_output("sudo chown -R {} {}".format(user, mountpoint).split())
    tsprint("Success")
    return 0


def tweak_vm(sysctl):
    subprocess.check_call("{sysctl} vm.dirty_expire_centisecs=30000".format(sysctl=sysctl).split())
    subprocess.check_call("{sysctl} vm.dirty_background_ratio=5".format(sysctl=sysctl).split())
    subprocess.check_call("{sysctl} vm.dirty_ratio=60".format(sysctl=sysctl).split())


def main_tweak_vm():
    try:
        tweak_vm(sysctl="sudo sysctl")
    except:
        tweak_vm(sysctl="sysctl")
    tsprint("Success")
    return 0


def main_raid(volume_name, *optional_args):
    if not volume_name.replace("_", "").replace("-", "").isalnum():
        tsprint("Invalid volume name argument: {}".format(volume_name))
        tsprint("Permitted characters: A..Z, a..z, -, _")
        return 1
    if len(optional_args) >= 1 and optional_args[0].startswith("/dev/"):
        return main_raid_nvme(volume_name, *optional_args)
    return main_raid_ebs(volume_name, *optional_args)


def initiate_fetch(s3_bucket, s3_key, part, n, file_size, request_tokens, errors):
    fetcher_subproc = None
    watchdog = None
    def kill():
        if fetcher_subproc:
            fetcher_subproc.terminate()
    def note_error():
        with errors[0]:
            errors[1] += 1
        tsprint("Fetching part '{}' failed.".format(part))
    def wait_and_release():
        try:
            if fetcher_subproc and fetcher_subproc.wait() != 0:
                note_error()
        except:
            note_error()
            raise
        finally:
            request_tokens.release()
            if watchdog:
                watchdog.cancel()
    try:
        first = segment_start(n, file_size)
        last = segment_start(n + 1, file_size) - 1  # aws api wants inclusive bounds
        command_str = "aws s3api get-object --range bytes={rfrom}-{rto} --bucket {bucket} --key {key} {part}".format(
            rfrom=first, rto=last, bucket=s3_bucket, key=s3_key, part=part)
        safe_remove(part)
        os.mkfifo(part)
        fetcher_subproc = subprocess.Popen(command_str.split(), stdout=DEVNULL)
    except:
        with errors[0]:
            errors[1] += 1
    finally:
        if fetcher_subproc:
            watchdog = threading.Timer(TIMEOUT, kill)
            watchdog.start()
        threading.Thread(target=wait_and_release).start()


def append(part, baton):
    with open(part, 'rb') as pf:
        segment_bytes = pf.read()
    baton.acquire()
    bytes_written = os.write(sys.stdout.fileno(), segment_bytes)
    assert bytes_written == len(segment_bytes)


def available_gigs_of_RAM():
    try:
        return int(check_output('fgrep MemAvailable /proc/meminfo'.split()).split()[1]) / (2.0**20)
    except:
        return None


def adjust_RAM_params():
    global MAX_SEGMENTS_IN_RAM
    global MAX_CONCURRENT_REQUESTS
    gigs = available_gigs_of_RAM()
    if gigs == None:
        # Mac laptop or tiny AWS instance => use 2GB RAM for buffers
        MAX_SEGMENTS_IN_RAM = 6
        MAX_CONCURRENT_REQUESTS = 3
    elif gigs <= 128:
        # Smallish AWS instance => use 6GB RAM for buffers
        MAX_SEGMENTS_IN_RAM = 16
        MAX_CONCURRENT_REQUESTS = 7
    elif gigs <= 384:
        # Medium AWS instance => use 12GB RAM for buffers
        MAX_SEGMENTS_IN_RAM = 32
        MAX_CONCURRENT_REQUESTS = 15


def main_cat(s3_uri):
    adjust_RAM_params()
    file_size = get_file_size(s3_uri)
    tsprint("File size is {:3.1f} GB ({} bytes).".format(file_size/(2**30), file_size))
    tsprint("Up to {gigs} GB of RAM will be used for buffers.".format(gigs=MAX_SEGMENTS_IN_RAM * SEGMENT_SIZE / (2.0 ** 30)))
    s3_bucket, s3_key = s3_bucket_and_key(s3_uri)
    N = num_segments(file_size)
    tsprint("Fetching {} segments.".format(N))
    active_appenders = Queue(MAX_SEGMENTS_IN_RAM)
    request_tokens = threading.Semaphore(MAX_CONCURRENT_REQUESTS)
    errors = [threading.RLock(), 0]
    def error_state():
        with errors[0]:
            return errors[1]
    def baton_passer_loop():
        while True:
            part, appender, baton = active_appenders.get()
            if appender == None:
                break
            try:
                baton.release()
                t0 = time.time()
                while appender.is_alive() and not error_state() and time.time() - t0 < TIMEOUT:
                    appender.join(5.0)
                if error_state() and appender.is_alive():
                    appender.terminate()
                assert appender.exitcode == 0
            except:
                with errors[0]:
                    errors[1] += 1
                tsprint("Error appending part '{}'.".format(part))
            finally:
                safe_remove(part)
    baton_passer = threading.Thread(target=baton_passer_loop)
    baton_passer.start()
    pid = os.getpid()
    def part_name(n):
        return part_filename("download-{}".format(pid), n, N)
    try:
        for n in range(N):
            request_tokens.acquire()
            if error_state():
                break
            part = part_name(n)
            baton = multiprocessing.Semaphore(1)
            baton.acquire()
            initiate_fetch(s3_bucket, s3_key, part, n, file_size, request_tokens, errors)
            appender = multiprocessing.Process(target=append, args=[part, baton])
            appender.start()
            active_appenders.put((part, appender, baton), block=True, timeout=TIMEOUT)
    finally:
        active_appenders.put((None, None, None))
        baton_passer.join()
        for n in range(N):
            safe_remove(part_name(n))
    if error_state():
        return 1
    return 0


def instance_availability_zone():
    "Return availability zone of current instance."
    return check_output("curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone".split())


def instance_id():
    "Return current instance id."
    return check_output("curl -s http://169.254.169.254/latest/meta-data/instance-id".split())


def create_volume(volume_name, volume_size, availability_zone, volume_type="gp2"):
    "Return volume id of newly created volume.  The volume may not be available yet."
    command = ("aws ec2 create-volume --volume-type {vt} --size {size} " +
               "--tag-specifications ResourceType=volume,Tags=[{{Key=Name,Value={vn}}}] --availability-zone {az}")
    command = command.format(vn=volume_name, az=availability_zone, size=volume_size, vt=volume_type)
    props = json.loads(check_output(command.split()))
    return props["VolumeId"]


def wait_until_state(volume_ids, predicate, timeout=300):
    "Wait up to timeout for all volume_ids to become available.  If the timeout expires, return False."
    command = "aws ec2 describe-volumes --volume-ids " + " ".join(volume_ids)
    t0 = time.time()
    sleep_quantum = 15.0
    while True:
        time.sleep(sleep_quantum)
        props = json.loads(check_output(command.split()))
        vstate = {}
        for v in props["Volumes"]:
            vstate[v["VolumeId"]] = predicate(v)
        if all((vid in vstate and vstate[vid]) for vid in volume_ids):
            # Success
            return True
        remaining_time = timeout - (time.time() - t0)
        if remaining_time < sleep_quantum:
            # Timeout
            return False


def main(argv):
    if len(argv) < 2 or argv[1] in ("help", "--help"):
        tsprint(help_text)
        return 0
    s3mi, command, args = argv[0], argv[1], argv[2:]
    assert s3mi.endswith("s3mi")
    if command == "cp":
        result = main_cp(*args)
    elif command == "cat":
        result = main_cat(*args)
    elif command == "raid":
        result = main_raid(*args)
    elif command in ("tweak_vm", "tweak-vm", "tweakvm"):
        result = main_tweak_vm(*args)
    else:
        raise Exception("Unsupported command '{}', see usage.".format(command))
    return result


def mainest():
    try:
        exitcode = main(sys.argv)
        if exitcode != 0:
            sys.exit(exitcode)
    except Exception as e:
        tsprint(str(e))
        tsprint(help_text)
        sys.exit(1)


if __name__ == "__main__":
    mainest()
